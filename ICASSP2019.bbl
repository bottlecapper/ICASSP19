\begin{thebibliography}{10}

\bibitem{mohammadi2017overview}
S.H. Mohammadi and A.~Kain,
\newblock ``An overview of voice conversion systems,''
\newblock {\em Speech Communication}, vol. 88, pp. 65--82, 2017.

\bibitem{zhao2018accent}
G.~Zhao, S.~Sonsaat, J.~Levis, E.~Chukharev-Hudilainen, and R.~Gutierrez-Osuna,
\newblock ``Accent conversion using phonetic posteriorgrams,''
\newblock in {\em ICASSP}. IEEE, 2018, pp. 5314--5318.

\bibitem{wang2012emotional}
M.~Wang, M.~Wen, K.~Hirose, and N.~Minematsu,
\newblock ``Emotional voice conversion for mandarin using tone nucleus
  model--small corpus and high efficiency,''
\newblock in {\em Speech Prosody 2012}, 2012.

\bibitem{wang2014multi}
Z.~Wang and Y.~Yu,
\newblock ``Multi-level prosody and spectrum conversion for emotional speech
  synthesis,''
\newblock in {\em Signal Processing (ICSP)}. IEEE, 2014, pp. 588--593.

\bibitem{xue2018voice}
Y.~Xue, Y.~Hamada, and M.~Akagi,
\newblock ``Voice conversion for emotional speech: Rule-based synthesis with
  degree of emotion controllable in dimensional space,''
\newblock {\em Speech Communication}, vol. 102, pp. 54--67, 2018.

\bibitem{kawahara1999restructuring}
H.~Kawahara, I.~Masuda-Katsuse, and A.~De~Cheveigne,
\newblock ``Restructuring speech representations using a pitch-adaptive
  time--frequency smoothing and an instantaneous-frequency-based f0 extraction:
  Possible role of a repetitive structure in sounds1,''
\newblock {\em Speech communication}, vol. 27, no. 3-4, pp. 187--207, 1999.

\bibitem{fujisaki1984analysis}
H.~Fujisaki and K.~Hirose,
\newblock ``Analysis of voice fundamental frequency contours for declarative
  sentences of japanese,''
\newblock {\em ASJs Japan (E)}, vol. 5, no. 4, pp. 233--242, 1984.

\bibitem{xue2016study}
Y.~Xue and M.~Akagi,
\newblock ``A study on applying target prediction model to parameterize power
  envelope of emotional speech,''
\newblock in {\em RISP workshop NCSP'16}. 信号処理学会, 2016.

\bibitem{gatys2016image}
L.A. Gatys, A.S. Ecker, and M.~Bethge,
\newblock ``Image style transfer using convolutional neural networks,''
\newblock in {\em CVPR}. IEEE, 2016, pp. 2414--2423.

\bibitem{Huang_2018_ECCV}
X.~Huang, MY. Liu, S.~Belongie, and J.~Kautz,
\newblock ``Multimodal unsupervised image-to-image translation,''
\newblock in {\em The European Conference on Computer Vision (ECCV)}, September
  2018.

\bibitem{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio,
\newblock ``Generative adversarial nets,''
\newblock in {\em NIPS}, 2014, pp. 2672--2680.

\bibitem{busso2008iemocap}
C.~Busso, M.~Bulut, CC~Lee, A.~Kazemzadeh, E.~Mower, S.~Kim, J.N. Chang,
  S.~Lee, and S.S. Narayanan,
\newblock ``Iemocap: Interactive emotional dyadic motion capture database,''
\newblock {\em Language resources and evaluation}, vol. 42, no. 4, pp. 335,
  2008.

\bibitem{kawanami2003gmm}
H.~Kawanami, Y.~Iwami, T.~Toda, H.~Saruwatari, and K.~Shikano,
\newblock ``Gmm-based voice conversion applied to emotional speech synthesis,''
\newblock in {\em Eurospeech}, 2003.

\bibitem{NIPS2017_6672}
M.Y. Liu, T.~Breuel, and J.~Kautz,
\newblock ``Unsupervised image-to-image translation networks,''
\newblock in {\em Advances in Neural Information Processing Systems (NIPS)},
  pp. 700--708. Curran Associates, Inc., 2017.

\bibitem{Hsu2017}
C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y.~Tsao, and H.-M. Wang,
\newblock ``Voice conversion from unaligned corpora using variational
  autoencoding wasserstein generative adversarial networks,''
\newblock in {\em Proc. Interspeech 2017}, 2017, pp. 3364--3368.

\bibitem{Kaneko2017}
Takuhiro Kaneko, Hirokazu Kameoka, Kaoru Hiramatsu, and Kunio Kashino,
\newblock ``Sequence-to-sequence voice conversion with similarity metric
  learned using generative adversarial networks,''
\newblock in {\em Proc. Interspeech 2017}, 2017, pp. 1283--1287.

\bibitem{fang2018high}
F.~Fang, J.~Yamagishi, I.~Echizen, and J.~Lorenzo-Trueba,
\newblock ``High-quality nonparallel voice conversion based on cycle-consistent
  adversarial network,''
\newblock {\em arXiv preprint arXiv:1804.00425}, 2018.

\bibitem{van2016wavenet}
A.~Van Den~Oord, S.~Dieleman, H.~Zen, K.~Simonyan, O.~Vinyals, A.~Graves,
  N.~Kalchbrenner, A.W. Senior, and K.~Kavukcuoglu,
\newblock ``Wavenet: A generative model for raw audio.,''
\newblock in {\em SSW}, 2016, p. 125.

\bibitem{tamamori2017speaker}
A.~Tamamori, T.~Hayashi, K.~Kobayashi, K.~Takeda, and T.~Toda,
\newblock ``Speaker-dependent wavenet vocoder,''
\newblock in {\em Proc. Interspeech}, 2017, vol. 2017, pp. 1118--1122.

\bibitem{huang2008three}
CF~Huang and M.~Akagi,
\newblock ``A three-layered model for expressive speech perception,''
\newblock {\em Speech Communication}, vol. 50, no. 10, pp. 810--828, 2008.

\bibitem{Zhu_2017_ICCV}
JY~Zhu, T.~Park, P.~Isola, and A.A. Efros,
\newblock ``Unpaired image-to-image translation using cycle-consistent
  adversarial networks,''
\newblock in {\em IEEE ICCV}, Oct 2017.

\bibitem{morise2016world}
M.~Morise, F.~Yokomori, and K.~Ozawa,
\newblock ``World: a vocoder-based high-quality speech synthesis system for
  real-time applications,''
\newblock {\em IEICE Trans. on Information and Systems}, vol. 99, no. 7, pp.
  1877--1884, 2016.

\bibitem{dauphin2017language}
Y.N. Dauphin, A.~Fan, M.~Auli, and D.~Grangier,
\newblock ``Language modeling with gated convolutional networks,''
\newblock in {\em ICML}, 2017, pp. 933--941.

\bibitem{mirsamadi2017automatic}
S.~Mirsamadi, E.~Barsoum, and C.~Zhang,
\newblock ``Automatic speech emotion recognition using recurrent neural
  networks with local attention,''
\newblock in {\em ICASSP}. IEEE, 2017, pp. 2227--2231.

\end{thebibliography}
